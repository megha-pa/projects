{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7690f909-4c6a-42e3-b403-0201b14eaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath \n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b5f52415-eded-4685-ad8d-50a0cc5d4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"demand-forecasting-330305\"\n",
    "DATE_COL = \"INVOICE_DATE\"\n",
    "PROD_COL = \"ARTICLE\"\n",
    "CUST_COL = \"PAYER\"\n",
    "TARGET_COL = \"BILLED_QTY\"\n",
    "FORECAST_HORIZON = 18\n",
    "BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "REGION=\"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "de63de5c-e31b-4f8a-899b-2ebd4ca6e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def query_generator() -> str:\n",
    "    try:\n",
    "        import logging\n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        DATE_COL = \"INVOICE_DATE\"\n",
    "        PROD_COL = \"ARTICLE\"\n",
    "        CUST_COL = \"PAYER\"\n",
    "        TARGET_COL = \"BILLED_QTY\"\n",
    "        FORECAST_HORIZON = 18\n",
    "        DATASET_ID=\"WLSPN_L1_prod\"\n",
    "        TABLE_ID=\"TBL_WUSA_ZSD9_SALES_VIEW\"\n",
    "        BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "        REGION=\"us-central1\"\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\"\n",
    "        query = f\"\"\"\n",
    "        SELECT {DATE_COL}, {PROD_COL}, {TARGET_COL},{CUST_COL}\n",
    "                FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` \n",
    "                where distribution_channel=40 and billed_qty > 0 and PAYER IN ('502687','502822','501870','502488','502782','502841','502623','502567','503149','500570','503227','503315','503054','502854','500811','503326','502503') and ARTICLE IN ('EFRN-TW-BHW-03','EWAY-TW-BHW-04','EFRN-TW-BHW-06','EWAY-TW-BHW-01','EFRN-TW-BHW-01','EJAM-TW-BHW-04','EJAM-TW-BHW-03','EJAM-TW-BHW-01','EFRN-TW-BHW-05','EFRN-TW-BHW-04','EHUD-TW-BHW-01',\n",
    " 'EWAY-TW-BHW-06','EHUD-TW-BHW-06','ECBA-TW-BTW3-05','EHUD-TW-BHW-02','EWAY-TW-BHW-05','EWAY-TW-BHW-02','EJAM-TW-BHW-06','ETUR-BTRG-RG12-01','EWHM-TW-8SET-09','ECBA-TW-BTW3-01','EWHM-TW-8SET-04','ECBA-TW-BTW3-07','EFRN-TW-BHW-15','EFRN-TW-BHW-02','MICP-BTRG-RG12-01','EAND-TW-BHW-02','EWAY-TW-BHW-09','EPLC-DUV-KING-01','EWHM-TW-8SET-06','ECBA-TW-BTW3-03','EAND-TW-BHW-01','EPLC-DUV-FLQN-01','EAND-TW-BHW-06','EWHM-TW-8SET-02','ETUR-BTRG-RG12-03','ECBA-TW-BTW3-04','ETUR-BTRG-RG04-01','MICP-BTRG-RG12-03',\n",
    " 'EFRN-TW-BHW-07','ETUR-BTRG-RG04-03','EJAM-TW-BHW9-03','EHUD-TW-BHW-04','EWAY-TW-BHW-10','EWAY-TW-BHW-07','EMDS-TW-BHW-04','EWAY-TW-BHW-08','EHUD-TW-BHW-03','EWHM-TW-8SET-01','MICP-BTRG-RG22-03','EAND-TW-BHW-03','ETUR-BTRG-RG04-05','EFRN-TW-BHW-08','ECBA-TW-BTW3-06','EFRN-TW-BHW-11','MICP-BTRG-RG22-01','MICP-BTRG-AS-03','EWHM-TW-8SET-03','EHUD-TW-BHW-05','EFRN-TW-BHW-12','EJAM-TW-BHW9-02','EPLC-DUV-FLQN-03','MICP-BTRG-RG12-02','ETUR-BTRG-RG12-05',\n",
    " 'EPLC-DUV-KING-02','EAND-TW-BHW-05','ETUR-BTRG-RG04-04','EJAM-TW-BHW-02','EFRN-TW-BHW-09','ETUR-BTRG-RG12-02','EFRN-TW-BHW-13','EFRN-TW-BS19-01','EFRN-TW-BHW-10','ETUR-BTRG-RG04-02','EAND-TW-BHW-04','EWHM-TW-8SET-07','EFRN-TW-BS19-06','ETUR-BTRG-RG12-04','EPLC-DUV-KING-03','EMDS-TW-BHW-06','ECCW-DUV-FLQN-02','EFRN-TW-BS19-03','EJAM-TW-BHW9-01','EMDS-TW-BHW-01','MICP-BTRG-RG22-02','ETUR-BTRG-RG12-06','MNST-TW-BHW-04','MICP-BTRG-AS-01','EWAY-TW-BHW-11','EJAM-TW-BHW-05','ETUR-BTRG-RG04-06','ECBB-TW-BHW-03','MNST-TW-BHW-01','EPLC-DUV-FLQN-02','EBBL-TW-BTW3-01','ECCW-DUV-FLQN-05','MICP-BTRG-AS-02','ECCW-DUV-FLQN-04','MNST-TW-BHW-05','ECBB-TW-BHW-01','EWHM-TW-8SET-08','EMDS-TW-BHW-02',\n",
    " 'ECBB-TW-BHW-06','ELFL-QST-QUEN-01','EWHM-TW-8SET-12','ECBA-TW-BTW3-02','MNST-TW-BHW-02','EWHM-TW-8SET-05','EFRN-TW-BS19-02','EIDL-TW-BHW-06','EJAM-TW-5BHW-03','EBTL-TW-BTW3-02','EPLC-DUV-FLQN-04','ECBB-TW-BHW-04','EFRN-TW-BS19-05','EMDS-TW-BHW-03','EFRN-TW-5BHW-01','EMDS-TW-BHW-05','EIDL-TW-BHW-04','EAND-TW-BHW-09')\n",
    "  ORDER BY {DATE_COL};\n",
    "        \"\"\"\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ERROR OCCURED IN COMPONENT : model_data_preperation :::: error message {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d868cbb9-f2c9-46c8-b68a-38acd6123106",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\"numpy\",\"pandas\", \"pyarrow\",\"tqdm\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def read_data_from_bq(\n",
    "    req_query: str,\n",
    "    output_bq_df_file_path: Output[Dataset]\n",
    "):\n",
    "    try:\n",
    "        from google.cloud import bigquery\n",
    "        import pandas as pd\n",
    "        from pprint import pprint\n",
    "        import logging\n",
    "        from tqdm import tqdm\n",
    "        from datetime import date, timedelta\n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        DATE_COL = \"INVOICE_DATE\"\n",
    "        PROD_COL = \"ARTICLE\"\n",
    "        CUST_COL = \"PAYER\"\n",
    "        TARGET_COL = \"BILLED_QTY\"\n",
    "        FORECAST_HORIZON = 18\n",
    "        ID_COL=\"CVC\"\n",
    "        BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "        REGION=\"us-central1\"\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\"\n",
    "        # declare the BigQuery Client\n",
    "        bq_client = bigquery.Client(project = PROJECT_ID)\n",
    "        # form the dataframe\n",
    "        df = bq_client.query(req_query).to_dataframe()\n",
    "\n",
    "       \n",
    "\n",
    "        #df.to_csv(output_bq_df_file_path.path,index = False)\n",
    "        \n",
    "        monthly_aggrigated_ecommerce_data=pd.DataFrame()\n",
    "        \n",
    "        for customer in tqdm(df[CUST_COL].unique()):\n",
    "            customer_df = df[(df[CUST_COL]==customer)]\n",
    "            for article in customer_df[PROD_COL].unique():\n",
    "                sub_df = customer_df[(customer_df[PROD_COL]==article)]\n",
    "                #end_date_last_day = sub_df[DATE_COL].max().replace(day=1) - timedelta(days=1)\n",
    "                \n",
    "\n",
    "                end_date_last_day = date.today().replace(day=1) - timedelta(days=1)\n",
    "                print(end_date_last_day)\n",
    "                end_date_last_day=pd.to_datetime(end_date_last_day)\n",
    "\n",
    "\n",
    "            #filter out the transactions or drop current month demand as it is progressively get updated\n",
    "                sub_df = sub_df[(sub_df[DATE_COL] <= end_date_last_day)]\n",
    "                if not(sub_df.empty):\n",
    "                    sub_df_copy = sub_df.copy()\n",
    "                    #add a record to fill 0s for discontinued product untill last month\n",
    "                    if(sub_df[DATE_COL].max() < end_date_last_day):\n",
    "                        record_to_insert = {CUST_COL : customer, PROD_COL: article, DATE_COL: end_date_last_day, TARGET_COL : 0} #populate a dummy record of last month so that resampling function handles filling 0s for discontinued products \n",
    "                        sub_df = sub_df.append(record_to_insert, ignore_index=True)\n",
    "\n",
    "                    #sub_df = sub_df.append(generate_forecast_window(end_date_last_day, FORECAST_HORIZON, article, customer, cost=sub_df[\"COST\"].iloc[-1]), ignore_index=True)\n",
    "                    #df['date'] = pd.to_datetime(df['date'])\n",
    "                    sub_df[DATE_COL]=pd.to_datetime(sub_df[DATE_COL])\n",
    "                    sub_df.index = sub_df[DATE_COL]\n",
    "                    sub_df = sub_df.resample(rule='MS').agg({'BILLED_QTY' : 'sum'})\n",
    "                    sub_df = sub_df.reset_index()\n",
    "                    sub_df[CUST_COL] = customer\n",
    "                    sub_df[PROD_COL] = article  \n",
    "                    sub_df[ID_COL]=sub_df[CUST_COL].map(str)+'_'+sub_df[PROD_COL].map(str)\n",
    "                    \n",
    "                    monthly_aggrigated_ecommerce_data = monthly_aggrigated_ecommerce_data.append(sub_df, ignore_index=True)\n",
    "\n",
    "        monthly_aggrigated_ecommerce_data.to_csv(output_bq_df_file_path.path,index = False)\n",
    "\n",
    "\n",
    "        def truncate_destination_tables(list_of_tables):\n",
    "                client = bigquery.Client(project=PROJECT_ID)\n",
    "                for table in list_of_tables:\n",
    "                    query_string = f\"\"\"\n",
    "                    TRUNCATE TABLE  {table};\n",
    "                    \"\"\"\n",
    "                    client.query(query_string).result()\n",
    "        DATASET_ID = 'mlops_ecommerse_data'    \n",
    "        list_of_destination_tables = [f\"{PROJECT_ID}.{DATASET_ID}.{'train_data'}\",f\"{PROJECT_ID}.{DATASET_ID}.{'test_data'}\"]\n",
    "        truncate_destination_tables(list_of_destination_tables)\n",
    "    except Exception as e:\n",
    "            logging.error(f\"ERROR OCCURED IN COMPONENT : custom training job :::: error message {e}\")\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0b81e04f-12be-449a-9274-a7fe1a32c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"numpy\",\"pandas\",\"google-cloud-bigquery==2.26.0\",\"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def train_valid_split(\n",
    "    input_file_path_from_bq: InputPath(\"Dataset\"), \n",
    "    train_df_path: Output[Dataset],\n",
    "    validation_df_path: Output[Dataset]\n",
    "):\n",
    "    try:\n",
    "        import pandas as pd \n",
    "        import numpy as np\n",
    "        import logging\n",
    "        from google.cloud import bigquery\n",
    "        from pprint import pprint\n",
    "        df = pd.read_csv(input_file_path_from_bq)\n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        DATE_COL = \"INVOICE_DATE\"\n",
    "        PROD_COL = \"ARTICLE\"\n",
    "        CUST_COL = \"PAYER\"\n",
    "        TARGET_COL = \"BILLED_QTY\"\n",
    "        FORECAST_HORIZON = 18\n",
    "        BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "        REGION=\"us-central1\"\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\"\n",
    "        ID_COL=\"CVC\"\n",
    "\n",
    "        def split_time_series_for_modeling(df, evaluation_size):\n",
    "            import pandas as pd \n",
    "            import numpy as np\n",
    "            train_size = len(df) - evaluation_size\n",
    "            train, eval = df.iloc[:train_size], df.iloc[train_size:]\n",
    "            return train, eval\n",
    "        train_df1=pd.DataFrame()\n",
    "        eval_df1=pd.DataFrame()\n",
    "        customer_articles_list_for_modeling = pd.DataFrame()\n",
    "        for customer in df[CUST_COL].unique():\n",
    "                    customer_df = df[df[CUST_COL] == customer]\n",
    "                    for article in df[PROD_COL].unique():\n",
    "                        single_article_df = customer_df[customer_df[PROD_COL] == article]\n",
    "                        #if(check_criteria_for_inital_modeling(single_article_df)):\n",
    "                        customer_articles_list_for_modeling = customer_articles_list_for_modeling.append({CUST_COL: customer, PROD_COL : article}, ignore_index=True)\n",
    "\n",
    "        for customer in customer_articles_list_for_modeling[CUST_COL].unique():\n",
    "                    customer_df = df[df[CUST_COL] == customer]\n",
    "                    article_results_df = pd.DataFrame()\n",
    "                    for article in customer_articles_list_for_modeling[PROD_COL].unique():\n",
    "                        single_article_df = customer_df[customer_df[PROD_COL] == article]\n",
    "                        if not(single_article_df.empty):\n",
    "                            train_df, evaluation_df = split_time_series_for_modeling(single_article_df, evaluation_size = 6) #Need to applied in component 2 and only train, test dfs need to be there\n",
    "                            train_df1=train_df1.append(train_df)\n",
    "                            eval_df1=eval_df1.append(evaluation_df)\n",
    "        #evaluation size is the number of months need to be considered for evalution set out of total data\n",
    "        #train_size = len(df) - evaluation_size\n",
    "        #train_df = df.iloc[:train_size]\n",
    "        #valid_df = df.iloc[train_size:]\n",
    "        train_df1.to_csv(train_df_path.path,index = False)\n",
    "        eval_df1.to_csv(validation_df_path.path,index = False)\n",
    "    except Exception as e:\n",
    "            logging.error(f\"ERROR OCCURED IN COMPONENT : custom training job :::: error message {e}\")\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    # return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a78a357e-8185-4e31-a598-18987759a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"numpy\",\"pandas\",\"google-cloud-storage\",\"google-cloud-bigquery==2.26.0\",\"pyarrow==4.0.1\",],\n",
    "    base_image=\"python:3.9\",\n",
    ")\n",
    "def save_dataset_to_gcs(\n",
    "    train_dataset: InputPath(\"Dataset\"),\n",
    "    validation_dataset: InputPath(\"Dataset\")\n",
    ") -> str:\n",
    "    try: \n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        import pandas as pd \n",
    "        from google.cloud import storage\n",
    "        from google.cloud import bigquery\n",
    "        from datetime import datetime\n",
    "        import logging\n",
    "        bigquery_train_table_name='demand-forecasting-330305.mlops_ecommerse_data.train_data'\n",
    "        bigquery_test_table_name='demand-forecasting-330305.mlops_ecommerse_data.test_data'\n",
    "\n",
    "        train_df = pd.read_csv(train_dataset)\n",
    "        valid_df = pd.read_csv(validation_dataset)\n",
    "\n",
    "\n",
    "        train_df.to_csv(\"train.csv\",index = False)\n",
    "        valid_df.to_csv(\"test.csv\", index = False)\n",
    "\n",
    "        def write_dataframe_to_bigquery_table(df, bigquery_full_table_name):\n",
    "            client = bigquery.Client(project=PROJECT_ID)\n",
    "            job = client.load_table_from_dataframe(df, bigquery_full_table_name)\n",
    "            job.result()\n",
    "\n",
    "        write_dataframe_to_bigquery_table(train_df, bigquery_train_table_name)\n",
    "        write_dataframe_to_bigquery_table(valid_df, bigquery_test_table_name)\n",
    "\n",
    "\n",
    "        def upload_to_gcs(file_to_upload):\n",
    "            TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "            dataset_path = \"data/{TIMESTAMP}/{file_name}\".format(file_name = file_to_upload,TIMESTAMP=TIMESTAMP)\n",
    "\n",
    "            storage_client = storage.Client(project = PROJECT_ID)\n",
    "\n",
    "            bucket = storage_client.get_bucket(\"welspun_mlops_data\")\n",
    "\n",
    "            blob = bucket.blob(dataset_path)\n",
    "\n",
    "            blob.upload_from_filename(file_to_upload)\n",
    "\n",
    "            return dataset_path\n",
    "\n",
    "        train_datasetpath=upload_to_gcs(\"train.csv\")\n",
    "        test_datasetpath=upload_to_gcs(\"test.csv\")\n",
    "\n",
    "        print(\"train_datasetpath\",train_datasetpath)\n",
    "        print(\"test_datasetpath\", test_datasetpath)\n",
    "    except Exception as e:\n",
    "            logging.error(f\"ERROR OCCURED IN COMPONENT : custom training job :::: error message {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "76aa5832-360f-481c-ad82-e7e01f25c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery==2.26.0\", \"pandas==1.3.0\", \"gcsfs==2021.6.1\", \"numpy==1.19.5\", \"pyarrow==4.0.1\", \"statsmodels==0.12.2\", \"scikit-learn==0.24.2\", \"pmdarima==1.8.2\", 'google-cloud-aiplatform==1.3.0',\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"custom_training_jobs_batch_submission.yaml\",\n",
    ")\n",
    "def custom_training_jobs_batch_submission(\n",
    "    input_file_path_from_bq: InputPath(\"Dataset\"),\n",
    "    train_dataset_path: InputPath(\"Dataset\"),\n",
    "    validation_dataset_path: InputPath(\"Dataset\"),\n",
    "    unique_id_path: Output[Dataset],\n",
    "    RUN_ID:str,\n",
    ") -> str:\n",
    "    \n",
    "    import time\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "    import datetime\n",
    "    import sys\n",
    "    import json\n",
    "    import gcsfs\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "        \n",
    "    start = time.time()\n",
    "    try: \n",
    "        \n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        DATE_COL = \"INVOICE_DATE\"\n",
    "        PROD_COL = \"ARTICLE\"\n",
    "        CUST_COL = \"PAYER\"\n",
    "        TARGET_COL = \"BILLED_QTY\"\n",
    "        FORECAST_HORIZON = 18\n",
    "        BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "        REGION=\"us-central1\"\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\"\n",
    "        ID_COL=\"CVC\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        PACKAGE_PATH_LIST =[\"gcr.io/demand-forecasting-330305/ma_package_final@sha256:ba1afe27110666701af8efa092257fdecc37f967ae68cd4294b7c561435230c6\",\n",
    "                           \"gcr.io/demand-forecasting-330305/ar_package_final@sha256:de120e816d38010cc0623715e4009a16f30c151ae3be17229d79c06113844900\",\n",
    "                            \"gcr.io/demand-forecasting-330305/sarimax_preestimate@sha256:7ddccccf41125da4c4014fc8d820ae3464ace0bd55e566f35aa5f6fdd3657363\",\n",
    "                            \"gcr.io/demand-forecasting-330305/arma_preestimate@sha256:065ef13830d3fc2ffeb911a115dd1422929a306a0f3bf8e7ababc1ab9024638f\",\n",
    "                            \"gcr.io/demand-forecasting-330305/arima_package@sha256:b65fabf0d5e7b0ca13a34af375308547d52155472e0b4ec2bc4f59282e571297\",\n",
    "                            \"gcr.io/demand-forecasting-330305/triple_expo_package@sha256:739fbe5b5ae665b8ba9b3910e6b0f03494a200fab492c42ff44b7f04cf08f0ec\",\n",
    "                            \"gcr.io/demand-forecasting-330305/double_exp_smothing@sha256:18fa6dd43b7c6a36fd8a96f7c4a3e8221c1fafcaf9440429f2131f2c85ea5fe3\"]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        def generate_batch(skus_list, bins):\n",
    "            \"\"\"\n",
    "            params - job_list : list of jobs to bin\n",
    "                     bins : default bins 10 jobs per batch\n",
    "            response - job_list_batches : dictionary containing batch_id, jobs_list pair \n",
    "            \"\"\"\n",
    "\n",
    "            skus_list_batches,lower, upper = {},0, bins\n",
    "            number_of_batches = len(skus_list)//bins\n",
    "            number_of_remaining_jobs = len(skus_list)%bins\n",
    "            if number_of_remaining_jobs > 0:\n",
    "                number_of_batches = number_of_batches + 1\n",
    "\n",
    "            for i in range(0,number_of_batches):\n",
    "                batch_id = \"batch_\" + str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S_')) + str(i+1)\n",
    "                if(i<number_of_batches):\n",
    "                    skus_list_batches[batch_id] = tuple(skus_list[lower:upper])\n",
    "                    lower, upper =  upper, upper+bins\n",
    "                else:\n",
    "                    skus_list_batches[batch_id] = tuple(skus_list[lower:])\n",
    "\n",
    "            return skus_list_batches\n",
    "\n",
    "        \n",
    "       \n",
    "        from google.cloud import aiplatform\n",
    "        from google.cloud.aiplatform import gapic as aip\n",
    "        #aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)\n",
    "\n",
    "        def sumbit_vertex_ai_job(cmd_args, job_name, model_dir,PACKAGE_PATH):\n",
    "            PROJECT_ID = \"demand-forecasting-330305\"\n",
    "            api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "            client_options = {\"api_endpoint\": api_endpoint}\n",
    "            client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "            custom_job = {\n",
    "                \"display_name\": job_name,\n",
    "                \"job_spec\": {\n",
    "                    \"worker_pool_specs\": [\n",
    "                        {\n",
    "                            \"machine_spec\": {\n",
    "                                \"machine_type\": \"n1-standard-16\",\n",
    "                                #\"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "                                #\"accelerator_count\": 2,\n",
    "                            },\n",
    "                            \"replica_count\": 1,\n",
    "                            \"container_spec\": {\n",
    "                                \"image_uri\": PACKAGE_PATH,\n",
    "                                \"command\": [],\n",
    "                                \"args\": cmd_args,\n",
    "                            },\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "            \n",
    "            parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "            response = client.create_custom_job(parent=parent, custom_job=custom_job)\n",
    "            print(\"name:\", response.name)\n",
    "        \n",
    "            return response\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.read_csv(input_file_path_from_bq)\n",
    "        train_df = pd.read_csv(train_dataset_path)\n",
    "        valid_df = pd.read_csv(validation_dataset_path)\n",
    "        total_sku=list(df[ID_COL].unique())\n",
    "        #total_sku=total_sku[0:100]\n",
    "        unique_job_id=[]\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "        skus_list_batches = generate_batch(total_sku,30)\n",
    "        #print(skus_list_batches)\n",
    "        #unique_job_id=pd.DataFrame()\n",
    "        unique_job_id = pd.DataFrame(columns=['JOB_ID'])\n",
    "        for PACKAGE_PATH_ALL in PACKAGE_PATH_LIST:\n",
    "            for batch_id, skus_list in skus_list_batches.items():\n",
    "                    CMDARGS = [\n",
    "                     \"--PROJECT_ID\",\"demand-forecasting-330305\",\n",
    "                     \"--gcs_uri_training_data\",train_dataset_path,\n",
    "                     \"--gcs_uri_evaluation_data\",validation_dataset_path,\n",
    "                     \"--destination_bigquery_table\",\"demand-forecasting-330305.mlops_ecommerse_data.CUSTOM_JOB_FINAL\",\n",
    "                     \"--FORECAST_HORIZON\",\"18\",  \n",
    "                     \"--CATEGORY_COL\",\"PAYER\",     \n",
    "                     \"--DATE_COL\",\"INVOICE_DATE\",      \n",
    "                     \"--PRODUCT_COL\",\"ARTICLE\",      \n",
    "                     \"--TARGET_COL\",\"BILLED_QTY\",\n",
    "                     \"--ID_COL\",\"CVC\",\n",
    "                     \"--RUNID_COL\",RUN_ID,\n",
    "                     \"--FORECAST_TYPE\",\"stastistical\",\n",
    "                     \"--SKUS_LIST\",str(skus_list)\n",
    "                    ]\n",
    "\n",
    "                    from datetime import datetime\n",
    "                   \n",
    "                    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "                    JOB_NAME = \"welspun_Ecommerse_job_\" + RUN_ID\n",
    "                    MODEL_DIR = \"{}/{}\".format(BUCKET_NAME, JOB_NAME)\n",
    "                    response=sumbit_vertex_ai_job(CMDARGS, JOB_NAME, MODEL_DIR,PACKAGE_PATH_ALL)\n",
    "                    #unique_job_id=unique_job_id.append(response.name)\n",
    "                    unique_job_id = unique_job_id.append({\"JOB_ID\": response.name}, ignore_index=True)\n",
    "                    #aa = aa.append({'A':l[i]}, ignore_index=True)\n",
    "                    logging.info(f\"Submitted {skus_list}\\n\\n\")\n",
    "                    print(unique_job_id[\"JOB_ID\"])\n",
    "                    time.sleep(5)\n",
    "            unique_job_id.to_csv(unique_id_path.path,index = False)\n",
    "                #train_df1.to_csv(train_df_path.path,index = False)\n",
    "    except Exception as e:\n",
    "            logging.error(f\"ERROR OCCURED IN COMPONENT : custom training job :::: error message {e}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9ca72de6-32bc-48da-a431-7f241d5f14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery==2.26.0\", \"pandas==1.3.0\", \"gcsfs==2021.6.1\", \"numpy==1.19.5\", \"pyarrow==4.0.1\", \"statsmodels==0.12.2\", \"scikit-learn==0.24.2\", \"pmdarima==1.8.2\", 'google-cloud-aiplatform==1.3.0',\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"custom_training_jobs_batch_submission.yaml\",\n",
    ")\n",
    "def custom_job_wait(unique_id_path: InputPath(\"Dataset\"),\n",
    ") -> str:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    try:\n",
    "        from google.cloud import bigquery\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import datetime\n",
    "        import sys\n",
    "        import json\n",
    "        import gcsfs\n",
    "        import logging\n",
    "        from google.cloud import storage\n",
    "        from google.cloud import aiplatform\n",
    "        from google.cloud.aiplatform import gapic as aip\n",
    "        \n",
    "        unique_job_id=pd.read_csv(unique_id_path)\n",
    "        for job_id in unique_job_id[\"JOB_ID\"].unique():\n",
    "\n",
    "                    def get_custom_job(name, silent=False):\n",
    "                        print(\"for loop started\")\n",
    "                        print(name)\n",
    "                        print(\"get_custom_started\")\n",
    "                        from google.cloud import aiplatform\n",
    "                        from google.cloud.aiplatform import gapic as aip\n",
    "                        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "                        api_endpoint = \"us-central1-aiplatform.googleapis.com\"\n",
    "\n",
    "                        client_options = {\"api_endpoint\": api_endpoint}\n",
    "                        client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "                        response = client.get_custom_job(name=name)\n",
    "                        if silent:\n",
    "                            return response\n",
    "\n",
    "                        print(\"name:\", response.name)\n",
    "                        #print(\"display_name:\", response.display_name)\n",
    "                        #print(\"state:\", response.state)\n",
    "                        #print(\"create_time:\", response.create_time)\n",
    "                        #print(\"update_time:\", response.update_time)\n",
    "                        return response\n",
    "\n",
    "\n",
    "                    response = get_custom_job(job_id)\n",
    "\n",
    "                    while True:\n",
    "                        print(\"while_started\")\n",
    "                        response = get_custom_job(job_id, True)\n",
    "                        if response.state != aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "                                print(\"Training job has not completed:\", response.state)\n",
    "                                print(\"Training job has not completed:\", response.name)\n",
    "\n",
    "                                if response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "                                    break\n",
    "                        else:\n",
    "                                #print(\"Training Job Time:\", response.end_time - response.start_time)\n",
    "                                #print(\"Training Elapsed Time:\", response.update_time - response.create_time)\n",
    "                                print(\"Training job completed:\", response.name)\n",
    "                                print(\"Training completed succesfully:\", response.state)\n",
    "                                break\n",
    "                        time.sleep(60)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ERROR OCCURED IN COMPONENT : no_demand_training :::: error message {e}\")\n",
    "    \n",
    "       \n",
    "    logging.info(\"waiting\")    \n",
    "    \n",
    "    return \"done\"\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "511c9952-75f5-4419-b92d-6ccd48d007c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery==2.26.0\", \"pandas==1.3.0\", \"gcsfs==2021.6.1\", \"numpy==1.19.5\", \"pyarrow==4.0.1\", \"statsmodels==0.12.2\", \"scikit-learn==0.24.2\", \"pmdarima==1.8.2\", 'google-cloud-aiplatform==1.3.0'],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"normal_demand_custom_training_jobs_batch_submission.yaml\",\n",
    ")\n",
    "def evaluation(status:str,RUN_ID: str,evaluation_df_path: Output[Dataset],\n",
    "):\n",
    "    try:\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error,r2_score\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from google.cloud import storage\n",
    "        from google.cloud import bigquery\n",
    "        from datetime import datetime\n",
    "        import logging\n",
    "        bigquery_evaluation_table_name='demand-forecasting-330305.mlops_ecommerse_data.EVALUATION_FINAL'\n",
    "\n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        DATE_COL = \"INVOICE_DATE\"\n",
    "        PROD_COL = \"ARTICLE\"\n",
    "        CUST_COL = \"PAYER\"\n",
    "        TARGET_COL = \"BILLED_QTY\"\n",
    "        FORECAST_HORIZON = 18\n",
    "        BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "        REGION=\"us-central1\"\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\"\n",
    "        ID_COL=\"CVC\" \n",
    "        predicted_col=\"BILLED_QTY_PREDICTED\"\n",
    "        MODEL_NAME=\"MODEL\"\n",
    "        TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        RUN_ID=f\"{RUN_ID}\"\n",
    "        #RUN_ID= \"\"\"+RUN_ID+\"\"\"\n",
    "        final_query = f\"\"\"\n",
    "            SELECT * from `{PROJECT_ID}.mlops_ecommerse_data.CUSTOM_JOB_FINAL` \n",
    "            ORDER BY {DATE_COL};\n",
    "            \"\"\"\n",
    "        bq_client = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "        modeling_df = bq_client.query(final_query).to_dataframe()\n",
    "        modeling_df.to_csv(\"modeling_df.csv\",index = False)\n",
    "        #forecast_df.to_csv(\"forecast_df.csv\",index = False)\n",
    "\n",
    "        #modeling_df = pd.read_csv(final_dataset)\n",
    "        modeling_df=modeling_df.fillna(0)\n",
    "        def upload_to_gcs(file_to_upload):\n",
    "\n",
    "                TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "                dataset_path = \"data/{TIMESTAMP}/{file_name}\".format(file_name = file_to_upload,TIMESTAMP=TIMESTAMP)\n",
    "\n",
    "                storage_client = storage.Client(project = PROJECT_ID)\n",
    "\n",
    "                bucket = storage_client.get_bucket(\"welspun_mlops_data\")\n",
    "\n",
    "                blob = bucket.blob(dataset_path)\n",
    "\n",
    "                blob.upload_from_filename(file_to_upload)\n",
    "\n",
    "        upload_to_gcs(\"modeling_df.csv\")\n",
    "\n",
    "        def write_dataframe_to_bigquery_table(df, bigquery_full_table_name):\n",
    "            print(\"WRITE Started!!!\")\n",
    "            client = bigquery.Client(project=PROJECT_ID)\n",
    "            job = client.load_table_from_dataframe(df, bigquery_full_table_name)\n",
    "            job.result()\n",
    "            print(\"WRITE SUCESSFUL!!!\")\n",
    "        \n",
    "        def welspun_accuracy_metric(actual, predicted):\n",
    "            if(actual==predicted):\n",
    "                return 100\n",
    "            try:\n",
    "                Error =  abs(predicted - actual) / max(predicted, actual)\n",
    "                return (1 - Error) * 100\n",
    "            except ZeroDivisionError:\n",
    "                print (\"zero division error occured\")\n",
    "                if(predicted<0):#if actual vs prediction is less than zero we are considering prediction is zero\n",
    "                    return 0\n",
    "\n",
    "\n",
    "\n",
    "                #evaluation_df = pd.DataFrame(columns=['RMSE','MAE','MAPE','MSE','R2_SCORE','MODEL','ID','TIMESTAMP'])\n",
    "        \n",
    "        evaluation_df = pd.DataFrame(columns=['RMSE','MAE','MAPE','MSE','R2_SCORE','MODEL','RUN_ID','CVC','ACCURACY'])  \n",
    "        #for CVC_ID in modeling_df[ID_COL].unique()\n",
    "        try:\n",
    "                for ID in modeling_df[ID_COL].unique():\n",
    "                        au = modeling_df[modeling_df[ID_COL] == ID]\n",
    "                        models_list=au[MODEL_NAME].unique()\n",
    "                        print(ID)\n",
    "                        print(models_list)\n",
    "                        for model in au[MODEL_NAME].unique():\n",
    "                            single_article_results = modeling_df[(modeling_df[ID_COL] == ID) & (modeling_df[MODEL_NAME] == model) & (modeling_df[\"RUN_ID\"] ==RUN_ID)]\n",
    "                            if not(single_article_results.empty):\n",
    "                                rmse = np.sqrt(mean_squared_error(single_article_results[TARGET_COL].to_list(), single_article_results[predicted_col].to_list()))\n",
    "                                mae = mean_absolute_error(single_article_results[TARGET_COL].to_list(), single_article_results[predicted_col].to_list())\n",
    "                                mape = mean_absolute_percentage_error(single_article_results[TARGET_COL].to_list(), single_article_results[predicted_col].to_list())\n",
    "                                mse = mean_squared_error(single_article_results[TARGET_COL].to_list(), single_article_results[predicted_col].to_list())\n",
    "                                pt=single_article_results.apply(lambda x : welspun_accuracy_metric(x[TARGET_COL],x[predicted_col]), axis=1)\n",
    "                                Accuracy=pt.mean()\n",
    "                                r_value = r2_score(single_article_results[TARGET_COL].to_list(), single_article_results[predicted_col].to_list())\n",
    "                                evaluation_df = evaluation_df.append({\"CVC\":ID,\"ACCURACY\":Accuracy,\"RMSE\":rmse,\"MAE\":mae,\"MAPE\":mape,\"MSE\":mse,\"R2_SCORE\":r_value,\"MODEL\":model,\"RUN_ID\":RUN_ID}, ignore_index=True)\n",
    "                                \n",
    " \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ERROR OCCURED IN COMPONENT : evaluation :::: error message {e}\")\n",
    "    \n",
    "        write_dataframe_to_bigquery_table(evaluation_df,bigquery_evaluation_table_name)\n",
    "\n",
    "        evaluation_df.to_csv(evaluation_df_path.path,index = False)\n",
    "        \n",
    "      \n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"ERROR OCCURED IN COMPONENT : evaluation :::: error message {e}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ff3d8e1e-374b-4e38-906c-7d66f7d49065",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery==2.26.0\", \"pandas==1.3.0\", \"gcsfs==2021.6.1\", \"numpy==1.19.5\", \"pyarrow==4.0.1\", \"statsmodels==0.12.2\", \"scikit-learn==0.24.2\", \"pmdarima==1.8.2\", 'google-cloud-aiplatform==1.3.0'],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"normal_demand_custom_training_jobs_batch_submission.yaml\",\n",
    ")\n",
    "def model_chapionship(evaluation_df_path: InputPath(\"Dataset\"),champ_df_path:Output[Dataset],RUN_ID: str,\n",
    "):\n",
    "    try:\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error,r2_score\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from google.cloud import storage\n",
    "        from google.cloud import bigquery\n",
    "        from datetime import datetime\n",
    "        import logging\n",
    "        from pandas.tseries.offsets import MonthEnd\n",
    "        PROJECT_ID = \"demand-forecasting-330305\"\n",
    "        DATE_COL = \"INVOICE_DATE\"\n",
    "        PROD_COL = \"ARTICLE\"\n",
    "        CUST_COL = \"PAYER\"\n",
    "        CUST_NAME_COL = \"CUSTOMER_NAME_\"\n",
    "        TARGET_COL = \"BILLED_QTY\"\n",
    "        FORECAST_HORIZON = 18\n",
    "        BUCKET_NAME = \"gs://welspun_mlops_data\"\n",
    "        REGION=\"us-central1\"\n",
    "        PIPELINE_ROOT = f\"{BUCKET_NAME}/welspun_project\"\n",
    "        ID_COL=\"CVC\" \n",
    "        predicted_col=\"BILLED_QTY_PREDICTED\"\n",
    "        MODEL_NAME=\"MODEL\"\n",
    "       \n",
    "        #RUN_ID= \"\"\"+RUN_ID+\"\"\"\n",
    "        TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        bigquery_forecast_error_table_name='demand-forecasting-330305.mlops_ecommerse_data.FORECAST_ERROR'\n",
    "        bigquery_rolling_error_table_name='demand-forecasting-330305.mlops_ecommerse_data.ROLLING_FORECAST'\n",
    "        bigquery_MERGED_Final_table_name='demand-forecasting-330305.mlops_ecommerse_data.champ_best_model'\n",
    "        bigquery_MERGED_TABLE_1_table_name='demand-forecasting-330305.mlops_ecommerse_data.champ_merged_final'\n",
    "        \n",
    "        def write_dataframe_to_bigquery_table(df, bigquery_full_table_name):\n",
    "            print(\"WRITE Started!!!\",bigquery_full_table_name)\n",
    "            client = bigquery.Client(project=PROJECT_ID)\n",
    "            job = client.load_table_from_dataframe(df, bigquery_full_table_name)\n",
    "            job.result()\n",
    "            print(\"WRITE SUCESSFUL!!!\")\n",
    "            \n",
    "        def execute_query(query_string):\n",
    "            print(query_string)\n",
    "            client = bigquery.Client(project=PROJECT_ID)\n",
    "            return (client.query(query_string).result().to_dataframe(create_bqstorage_client=True))\n",
    "\n",
    "        #RUN_ID=f\"{RUN_ID}\" \n",
    "        print(RUN_ID)\n",
    "        #RUN_ID=\"{}\".format(RUN_ID)\n",
    "        RUN_ID_final='\"'+RUN_ID+'\"'\n",
    "        print(RUN_ID)\n",
    "        merge_query_string = f\"\"\"\n",
    "            SELECT DISTINCT  *FROM `demand-forecasting-330305.mlops_ecommerse_data.CUSTOM_JOB_FINAL` AS model_data\n",
    "            LEFT JOIN  (SELECT  * FROM  `demand-forecasting-330305.mlops_ecommerse_data.EVALUATION_FINAL`) AS transaction_data \n",
    "            on (model_data.CVC = transaction_data.CVC AND model_data.MODEL = transaction_data.MODEL AND model_data.RUN_ID = transaction_data.RUN_ID)\n",
    "            where model_data.RUN_ID={RUN_ID_final}\n",
    "            \"\"\"\n",
    "        print(\"merge statement ran\")\n",
    "        merged_data  = execute_query(merge_query_string)   \n",
    "        merged_data['DT_KEY_FOR_END_DATE'] = pd.to_datetime(merged_data['INVOICE_DATE'],format=\"%Y%m%d\") + MonthEnd(0)\n",
    "        merged_data['DT_KEY_FOR_END_DATE']=merged_data['DT_KEY_FOR_END_DATE'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "        merged_data['DT_KEY_FOR_END_DATE']=merged_data['DT_KEY_FOR_END_DATE'].astype(int)\n",
    "      \n",
    "\n",
    "\n",
    "        print(\"merge finished\")\n",
    "        write_dataframe_to_bigquery_table(merged_data,bigquery_MERGED_TABLE_1_table_name)\n",
    "        final_merged_df = pd.DataFrame()\n",
    "        for ID in merged_data[ID_COL].unique() :\n",
    "            single_article_results = merged_data[(merged_data[ID_COL] == ID) & (merged_data['RUN_ID'] == RUN_ID)]\n",
    "            rslt_df_1 = single_article_results[single_article_results['ACCURACY']==single_article_results['ACCURACY'].max()] \n",
    "            final_merged_df = final_merged_df.append(rslt_df_1,ignore_index=True)\n",
    "            \n",
    "        \n",
    "        final_merged_df.to_csv(champ_df_path.path,index = False)\n",
    "        \n",
    "        write_dataframe_to_bigquery_table(final_merged_df, bigquery_MERGED_Final_table_name)\n",
    "        #RUN_ID=\"{}\".format(RUN_ID)\n",
    "        rolling_forecast_string=f\"\"\"\n",
    "            SELECT CVC,BILLED_QTY,BILLED_QTY_PREDICTED,BILLED_QTY_Forecasted,DT_KEY_IN,INVOICE_DATE,DT_KEY_FOR_END_DATE,DT_KEY_FOR_START_DATE,RUN_ID FROM  `demand-forecasting-330305.mlops_ecommerse_data.champ_merged_final` \n",
    "            where RUN_ID={RUN_ID_final}\n",
    "            \"\"\"\n",
    "        forecast_error_string=f\"\"\"\n",
    "            SELECT CVC,BILLED_QTY,BILLED_QTY_PREDICTED,BILLED_QTY_Forecasted,DT_KEY_IN,INVOICE_DATE,DT_KEY_FOR_END_DATE,DT_KEY_FOR_START_DATE,MODEL,RUN_ID,ACCURACY,RMSE,MAE,MAPE,MSE,R2_SCORE,FORECAST_TYPE FROM  `demand-forecasting-330305.mlops_ecommerse_data.champ_merged_final`  \n",
    "             where RUN_ID={RUN_ID_final}\n",
    "            \"\"\"\n",
    "           \n",
    "        forecast_data  = execute_query(forecast_error_string)\n",
    "        rolling_data  = execute_query(rolling_forecast_string)\n",
    "     \n",
    "       \n",
    "        write_dataframe_to_bigquery_table(rolling_data ,bigquery_rolling_error_table_name)\n",
    "        write_dataframe_to_bigquery_table(forecast_data, bigquery_forecast_error_table_name)\n",
    "        \n",
    "      \n",
    "    except Exception as e:\n",
    "        logging.error(f\"ERROR OCCURED IN COMPONENT : evaluation :::: error message {e}\")\n",
    "        \n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ce6f1d23-0a8a-4797-8715-f1abbc75ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ecee0f93-dae1-4b60-af75-fe6e3f8eda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID=\"run_id-{0}\".format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c8d566de-3213-45f6-bdc4-20849c215f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline.\n",
    "    name=\"pipeline-trial\",\n",
    ")\n",
    "def my_pipeline(RUN_ID:str=\"run_id-{0}\".format(TIMESTAMP)):  \n",
    "    query_generator_task = query_generator()\n",
    "    \n",
    "    read_data_from_bq_task = read_data_from_bq(query_generator_task.output)\n",
    "    \n",
    "    train_valid_split_task = train_valid_split(read_data_from_bq_task.outputs[\"output_bq_df_file_path\"])\n",
    "    \n",
    "    save_dataset_to_gcs_task = save_dataset_to_gcs(train_valid_split_task.outputs[\"train_df_path\"],train_valid_split_task.outputs[\"validation_df_path\"])\n",
    "    \n",
    "    custom_training_jobs_batch_submission_task=custom_training_jobs_batch_submission(read_data_from_bq_task.outputs[\"output_bq_df_file_path\"],train_valid_split_task.outputs[\"train_df_path\"],train_valid_split_task.outputs[\"validation_df_path\"],RUN_ID)\n",
    "    \n",
    "    custom_job_wait_task=custom_job_wait(custom_training_jobs_batch_submission_task.outputs[\"unique_id_path\"])\n",
    "    \n",
    "    evaluation_task=evaluation( custom_job_wait_task.output,RUN_ID)\n",
    "    \n",
    "    model_chapionship_task=model_chapionship(evaluation_task.outputs[\"evaluation_df_path\"],RUN_ID)\n",
    "    \n",
    "    \n",
    "    #save_forecasted_dataset_to_gcs_task=save_forecasted_dataset_to_gcs(evaluation_task.outputs[\"evaluation_df_path\"])\n",
    "    \n",
    "    \n",
    "    #train_model_task = train_model(read_data_from_bq_task.outputs[\"output_bq_df_file_path\"],train_valid_split_task.outputs[\"train_df_path\"],train_valid_split_task.outputs[\"validation_df_path\"])\n",
    "    \n",
    "    #load_final_data_to_gcs_task = load_final_data_to_gcs(train_model_task.outputs[\"final_df_path\"])\n",
    "    \n",
    "    #evaluation_task=evaluation(train_model_task.outputs[\"final_df_path\"])\n",
    "    \n",
    "    #write_evaluation_task=write_evaluation(evaluation_task.outputs[\"evaluation_df_path\"])\n",
    "    \n",
    "    \n",
    "    #load_data_task=load_data(train_model_task.outputs[\"final_df_path\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cdd2d4c3-5217-487c-beb7-4cf59cf8dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline, package_path=\"welspun-ecommerse-pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ef5ff1af-7e01-4232-b299-e0b99179d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID=\"run-id-{0}\".format(TIMESTAMP)\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"welspun-ecommerse-pipeline\",\n",
    "    template_path=\"welspun-ecommerse-pipeline.json\",\n",
    "    job_id=\"welspun-ecommerse-pipeline-{0}\".format(RUN_ID),\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0625a73-fae3-4ef5-a06c-cc03d03aa8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/welspun-ecommerse-pipeline-run-id-20220214083831?project=610719353811\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/610719353811/locations/us-central1/pipelineJobs/welspun-ecommerse-pipeline-run-id-20220214083831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f994b-5212-4498-9ad7-9815bfa516ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4fdfb-0ceb-4f64-ace0-ca4cb7446616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe72a1-8076-4421-9640-6a73fa530189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
